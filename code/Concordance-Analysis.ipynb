{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "82c08154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.text import Text\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4e118c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Popeyes</td>\n",
       "      <td>cZA_G7kIkyIrR15EKXoVFw</td>\n",
       "      <td>Awful.  Waited in line for 15-20 for a chicken...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Popeyes</td>\n",
       "      <td>zKMCLxQnAOXpHJIKMZCI_Q</td>\n",
       "      <td>This is my first time at the Hartsfield Airpor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Popeyes</td>\n",
       "      <td>CpRBM-El-mqvbv93lYX5QA</td>\n",
       "      <td>They weren't ready for the return of the chick...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Popeyes</td>\n",
       "      <td>ui1vL68Ty9_aeKGtzJNSHg</td>\n",
       "      <td>I have being several times there but they neve...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Popeyes</td>\n",
       "      <td>Lk_clm7vulcrkvcBAyO6fg</td>\n",
       "      <td>I have been trying to get the spicy chicken sa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name                 user_id  \\\n",
       "0  Popeyes  cZA_G7kIkyIrR15EKXoVFw   \n",
       "1  Popeyes  zKMCLxQnAOXpHJIKMZCI_Q   \n",
       "2  Popeyes  CpRBM-El-mqvbv93lYX5QA   \n",
       "3  Popeyes  ui1vL68Ty9_aeKGtzJNSHg   \n",
       "4  Popeyes  Lk_clm7vulcrkvcBAyO6fg   \n",
       "\n",
       "                                                text  stars  \n",
       "0  Awful.  Waited in line for 15-20 for a chicken...    1.0  \n",
       "1  This is my first time at the Hartsfield Airpor...    1.0  \n",
       "2  They weren't ready for the return of the chick...    4.0  \n",
       "3  I have being several times there but they neve...    1.0  \n",
       "4  I have been trying to get the spicy chicken sa...    1.0  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "data = pd.read_csv('all_chains_cs_reviews.csv')\n",
    "data['stars'] = data['stars_y']\n",
    "data = data.drop(['Unnamed: 0', 'business_id', 'city', 'address', 'state', 'postal_code', 'latitude', 'longitude', 'attributes', 'categories', 'hours', 'review_id', 'useful', 'funny', 'cool', 'date', 'is_open', 'stars_x', 'stars_y', 'review_count'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "915d7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1 and 2 star chicken sandwich reviews = 1097\n",
      "Number of 4 and 5 star chicken sandwich reviews = 1005\n"
     ]
    }
   ],
   "source": [
    "stars_1_2_reviews = pd.DataFrame(data[data['stars']==1.0])\n",
    "stars_1_2_reviews = stars_1_2_reviews.append(data[data['stars']==2.0])\n",
    "print(\"Number of 1 and 2 star chicken sandwich reviews =\", len(stars_1_2_reviews))\n",
    "stars_4_5_reviews = pd.DataFrame(data[data['stars']==4.0])\n",
    "stars_4_5_reviews = stars_4_5_reviews.append(data[data['stars']==5.0])\n",
    "print(\"Number of 4 and 5 star chicken sandwich reviews =\", len(stars_4_5_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9e20e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_1_2_text = \"\"\n",
    "stars_4_5_text = \"\"\n",
    "for i in range(len(stars_1_2_reviews)):\n",
    "    stars_1_2_text = stars_1_2_text + stars_1_2_reviews[\"text\"].iloc[i]\n",
    "\n",
    "for j in range(len(stars_4_5_reviews)):\n",
    "    stars_4_5_text = stars_4_5_text + stars_4_5_reviews[\"text\"].iloc[j]\n",
    "\n",
    "stars_1_2_text = re.sub(\"[^A-Za-z]+\", \" \", stars_1_2_text)\n",
    "stars_4_5_text = re.sub(\"[^A-Za-z]+\", \" \", stars_4_5_text)\n",
    "\n",
    "stars_1_2_text = Text(word_tokenize(stars_1_2_text))\n",
    "stars_4_5_text = Text(word_tokenize(stars_4_5_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "67bc7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "stars_1_2_con_list = stars_1_2_text.concordance_list([\"chicken\", \"sandwich\"], lines=1000)\n",
    "stars_4_5_con_list = stars_4_5_text.concordance_list([\"chicken\", \"sandwich\"], lines=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ad0d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all cases lowercase and add to string\n",
    "stars_1_2_string = \"\"\n",
    "stars_4_5_string = \"\"\n",
    "for i in range(len(stars_1_2_con_list)):\n",
    "    stars_1_2_con_list[i] = remove_stopwords(stars_1_2_con_list[i].line).lower()\n",
    "    stars_1_2_string = stars_1_2_string + stars_1_2_con_list[i]\n",
    "\n",
    "for i in range(len(stars_4_5_con_list)):\n",
    "    stars_4_5_con_list[i] = remove_stopwords(stars_4_5_con_list[i].line).lower()\n",
    "    stars_4_5_string = stars_4_5_string + stars_4_5_con_list[i]\n",
    "\n",
    "stars_1_2_string = re.sub('chicken sandwich', ' ', stars_1_2_string)\n",
    "stars_4_5_string = re.sub('chicken sandwich', ' ', stars_4_5_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a76feee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1_2 = word_tokenize(stars_1_2_string)\n",
    "fdist_1_2 = FreqDist(words_1_2)\n",
    "\n",
    "words_4_5 = word_tokenize(stars_4_5_string)\n",
    "fdist_4_5 = FreqDist(words_4_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "29596f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdist_1_2 = pd.DataFrame.from_dict(fdist_1_2, orient='index')\n",
    "df_fdist_1_2.columns = ['Frequency']\n",
    "df_fdist_1_2.index.name = 'Term'\n",
    "df_fdist_1_2 = df_fdist_1_2.sort_values(by=['Frequency'], ascending=False)\n",
    "df_fdist_1_2 = df_fdist_1_2[(df_fdist_1_2.Frequency != 1) & (df_fdist_1_2.Frequency != 2)]\n",
    "\n",
    "df_fdist_4_5 = pd.DataFrame.from_dict(fdist_4_5, orient='index')\n",
    "df_fdist_4_5.columns = ['Frequency']\n",
    "df_fdist_4_5.index.name = 'Term'\n",
    "df_fdist_4_5 = df_fdist_4_5.sort_values(by=['Frequency'], ascending=False)\n",
    "df_fdist_4_5 = df_fdist_4_5[(df_fdist_4_5.Frequency != 1) & (df_fdist_4_5.Frequency != 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "61aa2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdist_1_2 = df_fdist_1_2.drop([\"i\", \"the\", \"got\", \"s\", \"took\", \"w\", \"going\", \"and\", \"this\", \"bun\", \"ordered\", \"t\", \"order\", \"spicy\", \"grilled\", \"fries\", \"chicken\", \"like\", \"meal\", \"new\", \"crispy\", \"try\", \"food\", \"asked\", \"my\", \"it\", \"went\", \"they\", \"combo\", \"we\", \"home\", \"so\", \"know\", \"m\", \"king\", \"wife\", \"fried\", \"classic\", \"asiago\", \"not\", \"fil\", \"best\", \"she\", \"tried\", \"free\", \"burger\", \"pickles\", \"fresh\", \"sauce\", \"location\", \"lettuce\", \"instead\", \"wendy\", \"mayo\", \"buttermilk\", \"large\", \"regular\", \"ve\", \"received\", \"drive\", \"worst\", \"no\", \"came\", \"cheese\", \"when\", \"nuggets\", \"bacon\", \"wanted\", \"chick\", \"told\", \"a\", \"good\", \"tomato\", \"warm\", \"bit\", \"said\", \"today\",  \"original\", \"sandwich\", \"gave\"])\n",
    "df_fdist_1_2.head(20).to_csv('df_fdist_1_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "402bcd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdist_4_5 = df_fdist_4_5.drop([\"i\", \"the\", \"got\", \"s\", \"popeye\", \"hands\", \"right\", \"thought\", \"number\", \"come\", \"thing\", \"tenders\", \"salad\", \"biscuit\", \"comes\", \"enjoy\", \"fantastic\", \"with\", \"location\", \"spot\", \"there\", \"lunch\", \"wait\", \"wendy\", \"wrong\", \"ordered\", \"t\", \"order\", \"spicy\", \"grilled\", \"good\", \"best\", \"fries\", \"fil\", \"a\", \"love\", \"waffle\", \"chicken\", \"food\", \"deluxe\", \"chick\", \"original\", \"fried\", \"it\", \"better\", \"awesome\", \"they\", \"sauce\", \"place\", \"combo\", \"drive\", \"not\", \"but\", \"wheat\", \"definitely\", \"ice\", \"infamous\", \"sweet\", \"bun\", \"add\", \"tea\", \"ll\", \"and\", \"lettuce\", \"craving\", \"lemonade\", \"jack\", \"oh\", \"that\", \"day\", \"menu\", \"tasty\", \"famous\", \"finally\", \"know\", \"way\", \"today\", \"came\", \"pepper\", \"burger\", \"amazing\", \"delicious\", \"eat\", \"their\", \"we\", \"so\", \"usually\", \"favorite\", \"time\", \"classic\", \"popeyes\", \"sandwich\", \"this\", \"like\", \"m\", \"pickles\", \"cheese\", \"great\", \"try\", \"meal\", \"my\", \"regular\", \"ve\", \"nuggets\"])\n",
    "df_fdist_4_5.head(20).to_csv('df_fdist_4_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a6df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
